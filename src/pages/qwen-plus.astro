---
import BaseLayout from '../layouts/BaseLayout.astro';
const rawHtml = `
<p>
  <strong>Qwen-Plus</strong> is Alibaba Cloud's <strong>mid-tier API model</strong> — the balanced option between the budget-friendly Qwen-Flash and the frontier-grade <a href="/qwen-max/">Qwen-Max</a>. It offers strong reasoning, 1M-token context, and thinking mode at a fraction of the cost of competing frontier APIs. As of February 2026, the Plus tier includes <strong>Qwen3.5-Plus</strong> — powered by the newly released <a href="/qwen-3-5/">Qwen 3.5</a> open-source model — alongside the stable Qwen3-based <code>qwen-plus</code> alias.
</p>
<div class="ad-afterintro-container"><div class="ad-afterintro-inner"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9609544329602409" data-ad-slot="8310388095" data-ad-format="auto" data-full-width-responsive="true"></ins></div></div>

<div class="qwen-container" style="text-align:center;margin-bottom:30px;">
  <div class="qwen-row" style="justify-content:center;">
    <div class="qwen-col">
      <a class="qwen-button pro" href="https://dashscope-intl.aliyuncs.com/" target="_blank" rel="nofollow noopener noreferrer">
        <span class="button-content"><span class="button-text">Get API Key — Alibaba Cloud Model Studio</span></span>
      </a>
    </div>
  </div>
</div>

<div class="qwen-toc">
  <p class="qwen-toc-label">In This Guide</p>
  <ul>
    <li><a href="#what-is">What Is Qwen-Plus?</a></li>
    <li><a href="#tiers">API Tiers: Flash vs Plus vs Max</a></li>
    <li><a href="#specs">Specifications</a></li>
    <li><a href="#pricing">Pricing</a></li>
    <li><a href="#qwen35-plus">Qwen3.5-Plus (New)</a></li>
    <li><a href="#api-usage">API Quick Start</a></li>
    <li><a href="#versions">Version History</a></li>
    <li><a href="#when-to-use">When to Use Plus vs Max vs Flash</a></li>
    <li><a href="#faq">FAQ</a></li>
  </ul>
</div>


<h2 id="what-is">What Is Qwen-Plus?</h2>

<p>
  Qwen-Plus is <strong>not a specific open-source model</strong> — it's an API tier alias on Alibaba Cloud's Model Studio (DashScope). Think of it like OpenAI's "GPT-4o" vs "GPT-4o-mini" — different price/performance tiers pointing to different underlying models. The <code>qwen-plus</code> alias always points to a stable, production-ready snapshot that Alibaba updates periodically.
</p>
<p>
  The key advantage of Qwen-Plus over open-source self-hosting: you get <strong>1M-token context</strong>, <strong>thinking mode</strong>, and <strong>production-grade infrastructure</strong> without managing GPU servers. The key advantage over Qwen-Max: it's <strong>3× cheaper</strong> on input and significantly faster for most tasks.
</p>


<h2 id="tiers">API Tiers: Flash vs Plus vs Max</h2>

<p>
  Alibaba offers three main API tiers. Understanding the differences helps you pick the right one:
</p>

<div class="qwen-table-wrapper">
<table class="qwen-table">
<thead>
<tr>
  <th>Tier</th>
  <th>API Model ID</th>
  <th>Best For</th>
  <th>Context</th>
  <th>Thinking Mode</th>
  <th>Input $/1M</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Qwen-Flash</strong></td>
  <td><code>qwen-flash</code></td>
  <td>High-volume, low-latency, simple tasks</td>
  <td>1M</td>
  <td>No</td>
  <td>$0.05</td>
</tr>
<tr>
  <td><strong>Qwen-Plus</strong> ⭐</td>
  <td><code>qwen-plus</code></td>
  <td>Enterprise apps, RAG, general reasoning</td>
  <td>1M</td>
  <td>Yes</td>
  <td>$0.40</td>
</tr>
<tr>
  <td><strong>Qwen-Max</strong></td>
  <td><code>qwen3-max</code></td>
  <td>Complex reasoning, math, research</td>
  <td>262K</td>
  <td>Yes</td>
  <td>$1.20</td>
</tr>
</tbody>
</table>
</div>

<p>
  <strong>Note:</strong> Qwen-Turbo is deprecated. Alibaba recommends migrating to Qwen-Flash for equivalent low-cost use cases.
</p>


<h2 id="specs">Qwen-Plus Specifications</h2>

<div class="qwen-table-wrapper">
<table class="qwen-table">
<thead>
<tr>
  <th>Specification</th>
  <th>Value</th>
</tr>
</thead>
<tbody>
<tr><td><strong>Current stable snapshot</strong></td><td><code>qwen-plus-2025-12-01</code> (Qwen3-based)</td></tr>
<tr><td><strong>Latest snapshot</strong></td><td><code>qwen3.5-plus-2026-02-15</code> (Qwen3.5-based)</td></tr>
<tr><td><strong>Context window</strong></td><td>1,000,000 tokens</td></tr>
<tr><td><strong>Max output tokens</strong></td><td>32,768 tokens</td></tr>
<tr><td><strong>Thinking mode</strong></td><td>Yes (toggle via <code>enable_thinking</code>)</td></tr>
<tr><td><strong>Modalities — input</strong></td><td>Text (qwen-plus) / Text + Image + Video (qwen3.5-plus)</td></tr>
<tr><td><strong>Modalities — output</strong></td><td>Text only</td></tr>
<tr><td><strong>Languages</strong></td><td>100+ (qwen-plus) / 201 (qwen3.5-plus)</td></tr>
<tr><td><strong>Function calling</strong></td><td>Yes</td></tr>
<tr><td><strong>Structured output (JSON)</strong></td><td>Yes</td></tr>
<tr><td><strong>OpenAI-compatible API</strong></td><td>Yes</td></tr>
<tr><td><strong>License</strong></td><td>Proprietary (API access only)</td></tr>
</tbody>
</table>
</div>


<h2 id="pricing">Pricing</h2>

<h3>Qwen-Plus (stable, Qwen3-based)</h3>

<div class="qwen-table-wrapper">
<table class="qwen-table">
<thead>
<tr>
  <th>Context Range</th>
  <th>Input ($/1M)</th>
  <th>Output — Standard ($/1M)</th>
  <th>Output — Thinking ($/1M)</th>
</tr>
</thead>
<tbody>
<tr><td><strong>0–256K</strong></td><td>$0.40</td><td>$1.20</td><td>$4.00</td></tr>
<tr><td><strong>256K–1M</strong></td><td>$1.20</td><td>$3.60</td><td>$12.00</td></tr>
</tbody>
</table>
</div>

<h3>Qwen3.5-Plus (new, Qwen3.5-based)</h3>

<div class="qwen-table-wrapper">
<table class="qwen-table">
<thead>
<tr>
  <th>Context Range</th>
  <th>Input ($/1M)</th>
  <th>Output ($/1M)</th>
</tr>
</thead>
<tbody>
<tr><td><strong>0–256K</strong></td><td>$0.40</td><td>$2.40</td></tr>
<tr><td><strong>256K–1M</strong></td><td>$1.20</td><td>$7.20</td></tr>
</tbody>
</table>
</div>

<ul>
  <li><strong>Batch processing</strong>: 50% discount for asynchronous batch jobs.</li>
  <li><strong>Context caching</strong>: Implicit cache hits cost 20% of standard input; explicit cache hits cost 10%.</li>
  <li><strong>Free tier</strong>: 1 million tokens for new international accounts (valid 90 days).</li>
</ul>

<p>
  <strong>How does this compare?</strong> Qwen-Plus at $0.40/$1.20 per 1M tokens is roughly <strong>5–10× cheaper</strong> than GPT-4o and <strong>3× cheaper</strong> than <a href="/qwen-max/">Qwen-Max</a> on input costs. For most production workloads that don't require frontier-level reasoning, Plus offers the best cost-to-quality ratio in Alibaba's lineup.
</p>


<h2 id="qwen35-plus">Qwen3.5-Plus — The New Option</h2>

<p>
  Released on <strong>February 15, 2026</strong>, Qwen3.5-Plus is powered by the <a href="/qwen-3-5/">Qwen 3.5</a> open-source model (397B parameters, 17B active). Key differences from the standard <code>qwen-plus</code>:
</p>

<ul>
  <li><strong>Multimodal input</strong>: Accepts text, images, and video (standard qwen-plus is text-only).</li>
  <li><strong>201 languages</strong> (vs ~100 in standard qwen-plus).</li>
  <li><strong>Stronger benchmarks</strong>: Leads on IFBench, MathVision, OmniDocBench, and all agentic benchmarks.</li>
  <li><strong>No separate thinking surcharge</strong>: Same price regardless of thinking mode (standard qwen-plus charges 3.3× more for thinking output).</li>
  <li><strong>Slightly higher output cost</strong>: $2.40/M vs $1.20/M for standard mode — but no thinking surcharge makes it cheaper for reasoning tasks.</li>
</ul>

<p>
  <strong>Which should you use?</strong> If you need multimodal capabilities, agentic features, or run heavy reasoning tasks, <code>qwen3.5-plus</code> is the better choice. If you have a stable production pipeline that only needs text processing and you want the lowest cost, stick with <code>qwen-plus</code> until Alibaba updates the alias.
</p>

<h3>Model IDs</h3>
<ul>
  <li><code>qwen-plus</code> / <code>qwen-plus-latest</code> — Stable alias (currently points to Qwen3-based snapshot)</li>
  <li><code>qwen3.5-plus-2026-02-15</code> — Qwen 3.5 specific snapshot</li>
</ul>


<h2 id="api-usage">API Quick Start</h2>

<p>
  The API is <strong>OpenAI-compatible</strong>. You can use the OpenAI Python SDK with a different base URL:
</p>

<h3>Basic Text Request</h3>
<pre><code>from openai import OpenAI

client = OpenAI(
    api_key="YOUR_DASHSCOPE_API_KEY",
    base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
)

response = client.chat.completions.create(
    model="qwen-plus",  # or "qwen3.5-plus-2026-02-15"
    messages=[
        {"role": "user", "content": "Summarize the key risks in this contract..."}
    ]
)

print(response.choices[0].message.content)
</code></pre>

<h3>With Thinking Mode</h3>
<pre><code>response = client.chat.completions.create(
    model="qwen-plus",
    messages=[
        {"role": "user", "content": "Solve this optimization problem step by step..."}
    ],
    extra_body={"enable_thinking": True}
)
</code></pre>

<h3>Multimodal (Qwen3.5-Plus only)</h3>
<pre><code>response = client.chat.completions.create(
    model="qwen3.5-plus-2026-02-15",
    messages=[{
        "role": "user",
        "content": [
            {"type": "image_url", "image_url": {"url": "https://example.com/chart.png"}},
            {"type": "text", "text": "What does this chart show?"}
        ]
    }]
)
</code></pre>


<h2 id="versions">Version History</h2>

<p>
  The <code>qwen-plus</code> alias has pointed to different underlying models over time as Alibaba upgrades the backend:
</p>

<div class="qwen-table-wrapper">
<table class="qwen-table">
<thead>
<tr>
  <th>Snapshot</th>
  <th>Base Model</th>
  <th>Context</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr><td><code>qwen-plus-2025-01-25</code></td><td>Qwen 2.5</td><td>131K</td><td>Initial Qwen 2.5-era snapshot</td></tr>
<tr><td><code>qwen-plus-2025-04-28</code></td><td>Qwen 3</td><td>1M</td><td>Upgraded to Qwen 3 backbone, 1M context</td></tr>
<tr><td><code>qwen-plus-2025-09-11</code></td><td>Qwen 3</td><td>1M</td><td>Performance improvements</td></tr>
<tr><td><code>qwen-plus-2025-12-01</code></td><td>Qwen 3</td><td>1M</td><td>Current stable alias</td></tr>
<tr><td><code>qwen3.5-plus-2026-02-15</code></td><td><a href="/qwen-3-5/">Qwen 3.5</a></td><td>1M</td><td>Multimodal, 201 languages, new</td></tr>
</tbody>
</table>
</div>


<h2 id="when-to-use">When to Use Plus vs Max vs Flash</h2>

<div class="qwen-table-wrapper">
<table class="qwen-table">
<thead>
<tr>
  <th>Use Case</th>
  <th>Recommended Tier</th>
  <th>Why</th>
</tr>
</thead>
<tbody>
<tr><td>Customer support chatbot</td><td><strong>Flash</strong></td><td>Low latency, cheapest option, simple Q&A</td></tr>
<tr><td>RAG over long documents</td><td><strong>Plus</strong></td><td>1M context, good reasoning, cost-efficient</td></tr>
<tr><td>Enterprise analytics/reporting</td><td><strong>Plus</strong></td><td>Structured output, function calling, balanced cost</td></tr>
<tr><td>Image/video analysis</td><td><strong>Qwen3.5-Plus</strong></td><td>Only Plus-tier option with multimodal input</td></tr>
<tr><td>Agentic workflows</td><td><strong>Qwen3.5-Plus</strong></td><td>Best agentic benchmark scores in Plus tier</td></tr>
<tr><td>Math olympiad / competitive coding</td><td><strong>Max</strong></td><td>Test-time scaling, deepest reasoning</td></tr>
<tr><td>Scientific research / doctoral-level Q&A</td><td><strong>Max</strong></td><td>Highest GPQA, HLE scores</td></tr>
<tr><td>High-volume content generation</td><td><strong>Flash</strong></td><td>50× cheaper than Plus on input</td></tr>
</tbody>
</table>
</div>


<h2 id="faq">FAQ</h2>

<h3>Is Qwen-Plus the same as Qwen3.5-Plus?</h3>
<p>
  Not exactly. <code>qwen-plus</code> is the <strong>stable alias</strong> that Alibaba updates periodically (currently Qwen3-based). <code>qwen3.5-plus</code> is a <strong>specific named model</strong> based on the Qwen 3.5 architecture. They coexist — you can choose either via the API model ID.
</p>

<h3>Can I run Qwen-Plus locally?</h3>
<p>
  No — Qwen-Plus is API-only. However, the <strong>underlying models are open-source</strong>. The Qwen3.5-Plus API runs the same <a href="/qwen-3-5/">Qwen 3.5</a> model (397B MoE) that's available on HuggingFace under Apache 2.0. You can self-host it if you have the hardware (~256GB+ RAM). See our <a href="/run-locally/">Run Locally</a> guide.
</p>

<h3>How does Qwen-Plus compare to GPT-4o?</h3>
<p>
  Qwen-Plus offers comparable quality for most enterprise tasks at <strong>5–10× lower cost</strong>. The Qwen3.5-Plus variant adds multimodal capabilities and stronger benchmark scores than GPT-4o on instruction following and document understanding. For tasks requiring frontier reasoning (math, science), <a href="/qwen-max/">Qwen-Max</a> is the better comparison to GPT-5.
</p>

<h3>What happened to Qwen-Turbo?</h3>
<p>
  Deprecated. Alibaba recommends migrating to <strong>Qwen-Flash</strong>, which serves the same low-cost, high-speed niche at $0.05/M input tokens.
</p>
`;
---
<BaseLayout title="Qwen-Plus API — Alibaba Cloud's Mid-Tier Model" seoTitle="Qwen-Plus: Pricing, Specs & API Guide (2026)" seoDescription="Complete guide to Qwen-Plus API: pricing from $0.40/M tokens, 1M context, thinking mode, Qwen3.5-Plus multimodal. Compare with Max and Flash tiers.">
  <article class="qwen-container">
    <h1>Qwen-Plus: Alibaba Cloud's Mid-Tier API Model</h1>
    <Fragment set:html={rawHtml} />
  </article>
</BaseLayout>
