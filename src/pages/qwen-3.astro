---
import BaseLayout from '../layouts/BaseLayout.astro';
const rawHtml = "<p>\r\n  <strong>Qwen 3</strong> — released by <a href=\"https://www.alibabacloud.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Alibaba Cloud</a> on 28 April 2025 — redefines what an <em>open-source</em> large-language model can do.  From a pocket-friendly <strong>0.6 B-parameter</strong> model to the flagship <strong>235 B-parameter Mixture-of-Experts</strong>, every variant ships under the permissive\r\n  <a href=\"https://www.apache.org/licenses/LICENSE-2.0\" target=\"_blank\" rel=\"noopener noreferrer\">Apache 2.0 license</a>.  Trained on a colossal <strong>36 trillion-token</strong> corpus and fluent in <strong>119 languages</strong>, Qwen 3 competes head-to-head with GPT-4-class systems — yet remains fully transparent and free to fine-tune.  Looking for the bigger picture?  Explore the complete ecosystem on the <a href=\"/\">Qwen AI homepage</a>.\r\n</p>\r\n\r\n<div class=\"qwen-container\" style=\"text-align:center;margin-bottom:30px;\">\r\n  <div class=\"qwen-row\" style=\"justify-content:center;\">\r\n    <div class=\"qwen-col\">\r\n      <a class=\"qwen-button pro\" href=\"https://chat.qwen.ai/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">\r\n        <span class=\"button-content\"><span class=\"button-text\">Test-drive Qwen 3 now</span></span>\r\n      </a>\r\n    </div>\r\n  </div>\r\n</div>\r\n\r\n\r\n\r\n<p>\r\n  Stand-out upgrades include a dual-mode <strong>Hybrid Reasoning Engine</strong>, sparsely-activated MoE layers for maximum FLOPS-per-dollar, and monstrous context windows — <strong>32 K</strong> native and up to <strong>128 K</strong> with YaRN.  Below you’ll find deep-dive specs, benchmark results, and a step-by-step local install using <a href=\"https://ollama.com\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Ollama</a>. Prefer the hosted route?  Fire up <a href=\"/chat/\">Qwen AI Chat</a> and start exploring.\r\n</p>\r\n\r\n<center>\r\n  <img src=\"/wp-content/uploads/2025/04/qwen-3.webp\"\r\n       alt=\"Alibaba Cloud Qwen 3 model family overview\"\r\n       width=\"520\" height=\"272\" class=\"aligncenter size-full\"/>\r\n</center>\r\n\r\n<p><strong>Table of Contents</strong></p>\r\n<ul>\r\n  <li><a href=\"#qwen3-family-overview\">Qwen 3 Model Line-up</a></li>\r\n  <li><a href=\"#getting-started-qwen3\">Deployment Options</a></li>\r\n  <li><a href=\"#run-qwen3-ollama\">Local Install with Ollama</a></li>\r\n  <li><a href=\"#hardware-requirements-qwen3\">Hardware Guide</a></li>\r\n  <li><a href=\"#technical-architecture-innovations\">Architectural Highlights</a></li>\r\n  <li><a href=\"#hybrid-reasoning-engine\">Inside the Hybrid Reasoning Engine</a></li>\r\n  <li><a href=\"#training-methodology-qwen3\">Training & Alignment</a></li>\r\n  <li><a href=\"#context-length-mcp\">Context & Tool Calling (MCP)</a></li>\r\n  <li><a href=\"#qwen3-performance-benchmarks\">Benchmarks & Comparisons</a></li>\r\n  <li><a href=\"#use-cases-qwen3\">Prime Use Cases</a></li>\r\n  <li><a href=\"#conclusion-qwen3\">Next Steps</a></li>\r\n</ul>\r\n\r\n<h2 id=\"qwen3-family-overview\">Qwen 3 Model Line-up: Dense and MoE</h2>\r\n<p>\r\n  Choose from lean dense models for edge devices to heavyweight MoE titans for research labs.  All share identical tokenizer and instruction format, so you can swap models without rewriting code.\r\n</p>\r\n\r\n<h3>Dense Models (0.6 B → 32 B)</h3>\r\n<p>\r\n  Perfect for chatbots, real-time RAG, and multi-lingual assistants.  The 8 B variant runs in &lt;10 GB VRAM with GGUF quantization, yet scores &gt;80 on MT-Bench.\r\n</p>\r\n\r\n<h3>Mixture-of-Experts Variants</h3>\r\n<ul>\r\n  <li><strong>Qwen3-30B-A3B</strong> — 30.5 B total, ≈3.3 B active.  One high-end RTX 4090 handles it in real time.</li>\r\n  <li><strong>Qwen3-235B-A22B</strong> — 235 B total, 22 B active (8 of 128 experts/token).  Hits GPT-4-level reasoning while halving inference cost versus a monolithic dense model.</li>\r\n</ul>\r\n\r\n<h3>Specification Matrix</h3>\r\n<div class=\"qwen-table-container\">\r\n  <table>\r\n    <thead>\r\n      <tr>\r\n        <th>Model</th><th>Type</th><th>Total Params</th><th>Active Params</th>\r\n        <th>Native Ctx</th><th>YaRN Ctx</th>\r\n      </tr>\r\n    </thead>\r\n    <tbody>\r\n      <tr><td>Qwen3-0.6B</td><td>Dense</td><td>0.6 B</td><td>—</td><td>32 K</td><td>—</td></tr>\r\n      <tr><td>Qwen3-1.7B</td><td>Dense</td><td>1.7 B</td><td>—</td><td>32 K</td><td>—</td></tr>\r\n      <tr><td>Qwen3-4B</td><td>Dense</td><td>4 B</td><td>—</td><td>32 K</td><td>128 K</td></tr>\r\n      <tr><td>Qwen3-8B</td><td>Dense</td><td>8.2 B</td><td>—</td><td>32 K</td><td>128 K</td></tr>\r\n      <tr><td>Qwen3-14B</td><td>Dense</td><td>14 B</td><td>—</td><td>32 K</td><td>128 K</td></tr>\r\n      <tr><td>Qwen3-32B</td><td>Dense</td><td>32.8 B</td><td>—</td><td>32 K</td><td>128 K</td></tr>\r\n      <tr><td>Qwen3-30B-A3B</td><td>MoE</td><td>30.5 B</td><td>≈3.3 B</td><td>32 K</td><td>128 K</td></tr>\r\n      <tr><td>Qwen3-235B-A22B</td><td>MoE</td><td>235 B</td><td>≈22 B</td><td>32 K</td><td>128 K</td></tr>\r\n    </tbody>\r\n  </table>\r\n  <p style=\"font-size:0.9em;text-align:center;margin-top:5px;\">\r\n    <em>Official specs — April 2025 launch.</em>\r\n  </p>\r\n</div>\r\n\r\n<h2 id=\"getting-started-qwen3\">Getting Started with Qwen 3</h2>\r\n<p>\r\n  <strong>Cloud:</strong> Hit Qwen endpoints on Alibaba Cloud Model Studio / DashScope with OpenAI-compatible calls.<br/>\r\n  <strong>Local:</strong> Download weights from <a href=\"https://huggingface.co/Qwen\" target=\"_blank\" rel=\"noopener noreferrer\">Hugging Face</a>, <a href=\"https://www.modelscope.cn/models?name=qwen\" target=\"_blank\" rel=\"noopener noreferrer\">ModelScope</a> or GitHub, then serve with vLLM, llama.cpp, SGLang, or Ollama.  Quantised GGUF / AWQ / GPTQ builds slash VRAM needs by 70–80 %.\r\n</p>\r\n\r\n<h2 id=\"run-qwen3-ollama\">Run Qwen 3 Locally with Ollama</h2>\r\n<ol>\r\n  <li><strong>Install Ollama</strong> for Windows/macOS/Linux.</li>\r\n  <li><strong>Pull a model:</strong>\r\n    <code>ollama run qwen3:8b-q4_K_M</code> or\r\n    <code>ollama run qwen3:30b-a3b-q4_K_M</code>.\r\n  </li>\r\n  <li><strong>Chat:</strong> type your prompt and watch Qwen 3 think.</li>\r\n  <li><strong>Exit:</strong> <code>/bye</code>.</li>\r\n</ol>\r\n\r\n<h2 id=\"hardware-requirements-qwen3\">Hardware Requirements</h2>\r\n<ul>\r\n  <li><strong>0.6–1.7 B:</strong> 4 GB VRAM or even CPU-only for prototype bots.</li>\r\n  <li><strong>4–8 B:</strong> 8–16 GB VRAM — gaming laptop territory.</li>\r\n  <li><strong>14–32 B &amp; 30 B-MoE:</strong> 24 GB+ VRAM; runs comfortably on a single 4090 or A6000.</li>\r\n  <li><strong>235 B-MoE:</strong> multi-GPU H100 or A100 cluster; Kubernetes + vLLM recommended.</li>\r\n</ul>\r\n\r\n<h2 id=\"technical-architecture-innovations\">Architectural Highlights</h2>\r\n<p>\r\n  Qwen 3 keeps the Transformer backbone but layers on <strong>Grouped Query Attention</strong> for speed, <strong>QK-Norm</strong> for stability, and a 128-expert MoE grid with load-balancing loss to avoid expert collapse.  SwiGLU activations and RoPE-scaling extend context without hurting throughput.\r\n</p>\r\n\r\n<h2 id=\"hybrid-reasoning-engine\">Inside the Hybrid Reasoning Engine</h2>\r\n<p>\r\n  • <strong>Thinking Mode</strong> — emits explicit &lt;think&gt; chains, ideal for maths, STEM, multi-hop logic.<br/>\r\n  • <strong>Fast Mode</strong> — bypasses chain-of-thought to cut latency for support chat and search.<br/>\r\n  • <strong>Thinking Budget</strong> — hard-cap reasoning tokens: e.g. <code>max_thought_tokens=128</code>.\r\n</p>\r\n\r\n<h2 id=\"training-methodology-qwen3\">Training & Alignment</h2>\r\n<p>\r\n  <strong>Stage 1:</strong> 30 T mixed-quality web+code at 4 K seq → broad world knowledge.<br/>\r\n  <strong>Stage 2:</strong> 5 T high-quality STEM, coding &amp; reasoning → analytic strength.<br/>\r\n  <strong>Stage 3:</strong> 32 K seq long-context bootstrapping with hundreds of B tokens.<br/>\r\n  Post-training layers include multi-round SFT, reward-model RLHF, reasoning-focused RL and\r\n  on-policy distillation that shrinks compute by 90 % vs pure RL.\r\n</p>\r\n\r\n<h2 id=\"context-length-mcp\">Context and Model Context Protocol (MCP)</h2>\r\n<p>\r\n  Native 32 K context (GPU-friendly) extends to 128 K with YaRN.  MCP lets Qwen 3 call JSON-defined tools out-of-the-box:\r\n</p>\r\n<pre><code class=\"language-json\">{\r\n  \"name\": \"lookupWeather\",\r\n  \"description\": \"Get current weather\",\r\n  \"parameters\": { \"type\": \"object\", \"properties\": { \"city\": { \"type\": \"string\" } }, \"required\": [\"city\"] }\r\n}</code></pre>\r\n<p>\r\n  Your agent passes the call; Qwen 3 ingests the JSON response—no custom parsing needed.\r\n</p>\r\n\r\n<h2 id=\"qwen3-performance-benchmarks\">Benchmarks & Comparisons</h2>\r\n<p>\r\n  <strong>Qwen3-235B-A22B (thinking mode)</strong><br/>\r\n  • Arena-Hard 95.6 (≈Gemini 2.5 Pro)<br/>\r\n  • AIME’24 85.7 (math)<br/>\r\n  • LiveCodeBench 70.7 (coding)<br/>\r\n  • BFCL 70.8 (tool use)<br/>\r\n  <strong>Qwen3-30B-A3B</strong> hits Arena-Hard 91.0 at 1⁄8th the GPU budget of flagship dense models.  \r\n  Compact 8 B dense equals Llama 3 8 B on MMLU but beats it on C-Eval and MGSM.\r\n</p>\r\n\r\n<h2 id=\"use-cases-qwen3\">Prime Use Cases</h2>\r\n<ul>\r\n  <li><strong>Enterprise RAG:</strong> combine 128 K context with vector search for instant SOP answers.</li>\r\n  <li><strong>Autonomous Agents:</strong> MCP makes tool orchestration trivial.</li>\r\n  <li><strong>Code Co-pilots:</strong> 30 B-MoE solves 70 % of LiveCodeBench tasks out-of-the-box.</li>\r\n  <li><strong>Multilingual CX:</strong> 119-language fluency for support chat and localisation.</li>\r\n  <li><strong>Long-form Analysis:</strong> ingest 500-page PDFs without chunking.</li>\r\n  <li><strong>Robotics / Edge:</strong> 1.7 B dense on Jetson Orin for low-latency control.</li>\r\n</ul>\r\n\r\n<h2 id=\"conclusion-qwen3\">Next Steps</h2>\r\n<p>\r\n  Qwen 3 delivers open-source SOTA with commercial-grade licensing.  Fork it on GitHub, fine-tune on domain data, or scale instantly via Alibaba Cloud APIs.  Need multimodal?  Pair with <a href=\"/voice-video-chat/\">Qwen 2.5-Omni</\r\n";
---
<BaseLayout title="Qwen 3" seoTitle="Qwen 3 Guide: Try &amp; Deploy Alibaba’s Open-Source Hybrid LLM" seoDescription="Learn about Qwen 3. Specifications, benchmarks, features and how to use this AI model from Alibaba Cloud.">
  <article class="qwen-container">
    <h1>Qwen 3</h1>
    <Fragment set:html={rawHtml} />
  </article>
</BaseLayout>
