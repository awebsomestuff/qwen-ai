---
import BaseLayout from '../layouts/BaseLayout.astro';
const rawHtml = `
<p>
  <strong>Qwen3-Max-Thinking</strong> is Alibaba Cloud's most powerful AI model — a <strong>1+ trillion parameter</strong> Mixture-of-Experts system that tops multiple frontier benchmarks. Launched as <strong>Qwen3-Max</strong> in September 2025 and upgraded with <strong>test-time scaling (TTS)</strong> in January 2026, it scores a perfect 100% on AIME 2025, leads Humanity's Last Exam with search, and beats GPT-5.2 on math olympiad tasks. The model is <strong>API-only</strong> (proprietary), accessible through Alibaba Cloud's DashScope with OpenAI-compatible endpoints. For the complete Qwen 3 ecosystem, see the <a href="/qwen-3/">Qwen 3 overview</a>.
</p>
<div class="ad-afterintro-container"><div class="ad-afterintro-inner"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9609544329602409" data-ad-slot="8310388095" data-ad-format="auto" data-full-width-responsive="true"></ins></div></div>

<div class="qwen-container" style="text-align:center;margin-bottom:30px;">
  <div class="qwen-row" style="justify-content:center;">
    <div class="qwen-col">
      <a class="qwen-button pro" href="https://chat.qwen.ai/" target="_blank" rel="nofollow noopener noreferrer">
        <span class="button-content"><span class="button-text">Try Qwen3-Max-Thinking — chat.qwen.ai</span></span>
      </a>
    </div>
  </div>
</div>

<p>
  What sets Qwen3-Max-Thinking apart from other frontier models isn't just raw scale — it's <strong>how</strong> it uses that scale at inference time. The test-time scaling mechanism doesn't rely on naive best-of-N sampling. Instead, it employs an experience-cumulative, multi-round reasoning strategy that progressively refines answers across multiple internal passes. The result: state-of-the-art scores on the hardest reasoning benchmarks available, including several where it outperforms both GPT-5.2 and Gemini 3 Pro.
</p>

<div class="qwen-toc">
  <p class="qwen-toc-label">In This Guide</p>
  <div class="qwen-toc-grid">
    <a href="#key-specs" class="qwen-toc-chip">Key Specs</a>
    <a href="#test-time-scaling" class="qwen-toc-chip qwen-toc-highlight">Test-Time Scaling</a>
    <a href="#benchmarks" class="qwen-toc-chip">Benchmarks</a>
    <a href="#vs-frontier" class="qwen-toc-chip">vs Frontier Models</a>
    <a href="#strengths-weaknesses" class="qwen-toc-chip">Strengths & Gaps</a>
    <a href="#api-access" class="qwen-toc-chip">API Access</a>
    <a href="#pricing" class="qwen-toc-chip">Pricing</a>
    <a href="#use-cases" class="qwen-toc-chip">Use Cases</a>
    <a href="#limitations" class="qwen-toc-chip">Limitations</a>
    <a href="#timeline" class="qwen-toc-chip">Timeline</a>
    <a href="#faq" class="qwen-toc-chip">FAQ</a>
  </div>
</div>

<h2 id="key-specs">Key Specifications</h2>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <tbody>
      <tr>
        <td><strong>Developer</strong></td>
        <td>Alibaba Cloud — Qwen Team</td>
      </tr>
      <tr>
        <td><strong>Model Name</strong></td>
        <td>Qwen3-Max-Thinking (snapshot: qwen3-max-2026-01-23)</td>
      </tr>
      <tr>
        <td><strong>Parameters</strong></td>
        <td>1+ trillion (Mixture-of-Experts)</td>
      </tr>
      <tr>
        <td><strong>Architecture</strong></td>
        <td>MoE with global-batch load-balancing loss</td>
      </tr>
      <tr>
        <td><strong>Context Window</strong></td>
        <td>256,000 tokens (up to 1M referenced)</td>
      </tr>
      <tr>
        <td><strong>Max Output Tokens</strong></td>
        <td>131,072 tokens</td>
      </tr>
      <tr>
        <td><strong>Languages</strong></td>
        <td>100+</td>
      </tr>
      <tr>
        <td><strong>Pre-training Data</strong></td>
        <td>36 trillion tokens</td>
      </tr>
      <tr>
        <td><strong>Training Method</strong></td>
        <td>Two-stage: fine-tuning + reinforcement learning</td>
      </tr>
      <tr>
        <td><strong>Thinking Mode</strong></td>
        <td>Toggle via <code>enable_thinking</code> API parameter</td>
      </tr>
      <tr>
        <td><strong>Output Speed</strong></td>
        <td>~38 tokens/second</td>
      </tr>
      <tr>
        <td><strong>License</strong></td>
        <td>Proprietary (API-only, closed source)</td>
      </tr>
      <tr>
        <td><strong>API Compatibility</strong></td>
        <td>OpenAI-compatible + Anthropic-compatible</td>
      </tr>
      <tr>
        <td><strong>Initial Release</strong></td>
        <td>September 5, 2025 (Qwen3-Max)</td>
      </tr>
      <tr>
        <td><strong>TTS Upgrade</strong></td>
        <td>January 27, 2026 (Qwen3-Max-Thinking)</td>
      </tr>
    </tbody>
  </table>
</div>

<h2 id="test-time-scaling">Test-Time Scaling: How It Works</h2>
<p>
  Test-time scaling (TTS) is the key innovation that elevates Qwen3-Max from a strong model to a benchmark leader. Unlike standard inference where the model generates one answer, TTS allows the model to <strong>trade compute for intelligence at inference time</strong>.
</p>
<p>
  The mechanism works through what Alibaba describes as an <strong>experience-cumulative, multi-round reasoning strategy</strong>:
</p>
<ul>
  <li><strong>Multi-pass refinement:</strong> The model revisits and refines its reasoning across multiple internal rounds, building on previous attempts rather than starting from scratch each time.</li>
  <li><strong>Adaptive tool integration:</strong> During reasoning, the model can invoke built-in tools — search, memory, and a code interpreter — to gather additional information or verify computations.</li>
  <li><strong>Controllable latency:</strong> Developers can adjust the depth of reasoning to balance accuracy against response time. Lighter tasks skip deep reasoning; hard problems get the full treatment.</li>
</ul>
<p>
  The practical effect is dramatic. On AIME 2025 (American Invitational Mathematics Examination), standard mode scores 81.6 — but with test-time scaling enabled, the score jumps to a <strong>perfect 100%</strong>. Similar gains appear across GPQA Diamond (+5.4 points), LiveCodeBench (+5.5), and HLE with search (+8.5).
</p>

<h3>Dual Mode Operation</h3>
<p>
  Qwen3-Max-Thinking operates in two modes, controlled by the <code>enable_thinking</code> API parameter:
</p>
<ul>
  <li><strong>Thinking Mode</strong> (<code>enable_thinking: true</code>) — Activates chain-of-thought reasoning and test-time scaling. Lower temperature and top_p settings are recommended. Best for complex math, STEM, multi-hop reasoning, and agentic tasks.</li>
  <li><strong>Non-Thinking Mode</strong> (<code>enable_thinking: false</code>) — Fast direct responses without chain-of-thought. Ideal for general chat, search, customer support, and latency-sensitive applications.</li>
</ul>

<h2 id="benchmarks">Complete Benchmark Results</h2>

<h3>Standard Mode (Without Test-Time Scaling)</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Benchmark</th>
        <th>Score</th>
        <th>Category</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>GPQA Diamond</td><td>87.4</td><td>PhD-level Science</td></tr>
      <tr><td>MMLU-Pro</td><td>85.7</td><td>Knowledge</td></tr>
      <tr><td>MMLU-Redux</td><td>92.8</td><td>Knowledge</td></tr>
      <tr><td>C-Eval</td><td>93.7</td><td>Chinese Knowledge</td></tr>
      <tr><td>HLE</td><td>30.2</td><td>Extreme Difficulty</td></tr>
      <tr><td>HLE (with search)</td><td>49.8</td><td>Agentic Reasoning</td></tr>
      <tr><td>LiveCodeBench v6</td><td>85.9</td><td>Coding</td></tr>
      <tr><td>HMMT Feb 25</td><td>98.0</td><td>Math Competition</td></tr>
      <tr><td>HMMT Nov 25</td><td>94.7</td><td>Math Competition</td></tr>
      <tr><td>IMO-AnswerBench</td><td>83.9</td><td>Math Olympiad</td></tr>
      <tr><td>SWE-Bench Verified</td><td>75.3</td><td>Software Engineering</td></tr>
      <tr><td>Arena-Hard v2</td><td>90.2</td><td>General Chat</td></tr>
      <tr><td>IFBench</td><td>70.9</td><td>Instruction Following</td></tr>
      <tr><td>BFCL-V4</td><td>67.7</td><td>Function Calling</td></tr>
      <tr><td>Tau2-Bench</td><td>82.1</td><td>Agent Tool Use</td></tr>
      <tr><td>SuperGPQA</td><td>65.1</td><td>Graduate-level</td></tr>
    </tbody>
  </table>
</div>

<h3>With Test-Time Scaling (Heavy Mode)</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Benchmark</th>
        <th>Without TTS</th>
        <th>With TTS</th>
        <th>Gain</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>AIME 2025</td><td>81.6</td><td><strong>100.0</strong></td><td>+18.4</td></tr>
      <tr><td>HMMT</td><td>98.0</td><td><strong>100.0</strong></td><td>+2.0</td></tr>
      <tr><td>GPQA Diamond</td><td>87.4</td><td><strong>92.8</strong></td><td>+5.4</td></tr>
      <tr><td>IMO-AnswerBench</td><td>83.9</td><td><strong>91.5</strong></td><td>+7.6</td></tr>
      <tr><td>LiveCodeBench v6</td><td>85.9</td><td><strong>91.4</strong></td><td>+5.5</td></tr>
      <tr><td>HLE</td><td>30.2</td><td><strong>36.5</strong></td><td>+6.3</td></tr>
      <tr><td>HLE (with search)</td><td>49.8</td><td><strong>58.3</strong></td><td>+8.5</td></tr>
    </tbody>
  </table>
  <p style="font-size:0.9em;text-align:center;margin-top:5px;">
    <em>Test-time scaling consistently adds 2–18 points across all benchmarks tested.</em>
  </p>
</div>

<h2 id="vs-frontier">Head-to-Head: Qwen3-Max-Thinking vs Frontier Models</h2>

<h3>Science &amp; Reasoning</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr><th>Benchmark</th><th>Qwen3-Max (TTS)</th><th>GPT-5.2</th><th>Gemini 3 Pro</th><th>Claude Opus 4.5</th><th>DeepSeek V3.2</th></tr>
    </thead>
    <tbody>
      <tr><td>GPQA Diamond</td><td><strong>92.8</strong></td><td>92.4</td><td>91.9</td><td>87.0</td><td>82.4</td></tr>
      <tr><td>HLE (no search)</td><td>36.5</td><td>35.5</td><td><strong>37.5</strong></td><td>30.8</td><td>25.1</td></tr>
      <tr><td>HLE (with search)</td><td><strong>58.3</strong></td><td>45.5</td><td>45.0</td><td>43.2</td><td>40.8</td></tr>
    </tbody>
  </table>
</div>

<h3>Mathematics</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr><th>Benchmark</th><th>Qwen3-Max (TTS)</th><th>GPT-5.2</th><th>Gemini 3 Pro</th><th>Claude Opus 4.5</th><th>DeepSeek V3.2</th></tr>
    </thead>
    <tbody>
      <tr><td>IMO-AnswerBench</td><td><strong>91.5</strong></td><td>86.3</td><td>83.3</td><td>84.0</td><td>78.3</td></tr>
      <tr><td>HMMT Feb 25</td><td><strong>98.0</strong></td><td>—</td><td>97.5</td><td>—</td><td>92.5</td></tr>
      <tr><td>AIME 2025</td><td><strong>100.0</strong></td><td>—</td><td>—</td><td>—</td><td>—</td></tr>
    </tbody>
  </table>
</div>

<h3>Coding &amp; Software Engineering</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr><th>Benchmark</th><th>Qwen3-Max (TTS)</th><th>GPT-5.2</th><th>Gemini 3 Pro</th><th>Claude Opus 4.5</th><th>DeepSeek V3.2</th></tr>
    </thead>
    <tbody>
      <tr><td>LiveCodeBench v6</td><td><strong>91.4</strong></td><td>87.7</td><td>90.7</td><td>84.8</td><td>80.8</td></tr>
      <tr><td>SWE-Bench Verified</td><td>75.3</td><td>80.0</td><td>76.2</td><td><strong>80.9</strong></td><td>73.1</td></tr>
    </tbody>
  </table>
</div>

<h3>Agents &amp; General Quality</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr><th>Benchmark</th><th>Qwen3-Max (TTS)</th><th>GPT-5.2</th><th>Gemini 3 Pro</th><th>Claude Opus 4.5</th><th>DeepSeek V3.2</th></tr>
    </thead>
    <tbody>
      <tr><td>Arena-Hard v2</td><td><strong>90.2</strong></td><td>—</td><td>—</td><td>76.7</td><td>—</td></tr>
      <tr><td>Tau2-Bench</td><td>82.1</td><td>80.9</td><td>85.4</td><td><strong>85.7</strong></td><td>80.3</td></tr>
    </tbody>
  </table>
</div>

<h2 id="strengths-weaknesses">Where Qwen3-Max Leads — and Where It Doesn't</h2>

<h3>Clear Advantages</h3>
<div class="qwen-grid qwen-grid-cols-2">
  <div class="feature-item"><strong>Math Olympiad dominance</strong> — #1 on IMO-AnswerBench (91.5), perfect AIME 2025, and near-perfect HMMT (98.0/100.0)</div>
  <div class="feature-item"><strong>HLE with search</strong> — 58.3 on Humanity's Last Exam when tools are available, 13 points ahead of GPT-5.2</div>
  <div class="feature-item"><strong>Competitive coding</strong> — 91.4 on LiveCodeBench v6, the highest among all models tested</div>
  <div class="feature-item"><strong>General chat quality</strong> — 90.2 on Arena-Hard v2 and ~1430 ELO on the LMArena leaderboard</div>
</div>

<h3>Notable Gaps</h3>
<ul>
  <li><strong>SWE-Bench Verified (75.3):</strong> Real-world software engineering trails Claude Opus 4.5 (80.9) and GPT-5.2 (80.0). Competitive coding skill doesn't fully translate to multi-file debugging.</li>
  <li><strong>Tau2-Bench (82.1):</strong> Agent tool use falls behind Claude Opus 4.5 (85.7) and Gemini 3 Pro (85.4).</li>
  <li><strong>Speed (~38 tokens/s):</strong> One of the slower frontier models. The test-time scaling mode adds further latency.</li>
  <li><strong>HLE without search (36.5):</strong> Slightly behind Gemini 3 Pro (37.5) when tools aren't available.</li>
</ul>

<h2 id="api-access">API Access</h2>
<p>
  Qwen3-Max-Thinking is available through <strong>Alibaba Cloud's DashScope / Model Studio</strong> service with OpenAI-compatible API endpoints. It's also available on third-party platforms:
</p>
<ul>
  <li><strong>DashScope (Official)</strong> — Base URL: <code>https://dashscope-intl.aliyuncs.com/compatible-mode/v1</code></li>
  <li><strong>OpenRouter</strong> — Available as <code>qwen/qwen3-max</code></li>
  <li><strong>Novita AI</strong> — Lower pricing option</li>
</ul>

<h3>Quick Start (Python)</h3>
<pre><code class="language-python">from openai import OpenAI

client = OpenAI(
    base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
    api_key="your-dashscope-api-key"
)

response = client.chat.completions.create(
    model="qwen3-max-2026-01-23",
    messages=[{"role": "user", "content": "Prove that sqrt(2) is irrational."}],
    extra_body={"enable_thinking": True}
)
print(response.choices[0].message.content)</code></pre>

<h2 id="pricing">Pricing</h2>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr><th>Provider</th><th>Input</th><th>Output</th><th>Cache Read</th></tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>DashScope (≤128K)</strong></td>
        <td>$1.20 / 1M tokens</td>
        <td>$6.00 / 1M tokens</td>
        <td>$0.24 / 1M</td>
      </tr>
      <tr>
        <td><strong>DashScope (&gt;128K)</strong></td>
        <td>$3.00 / 1M tokens</td>
        <td>$15.00 / 1M tokens</td>
        <td>$0.60 / 1M</td>
      </tr>
      <tr>
        <td><strong>Novita AI</strong></td>
        <td>$0.50 / 1M tokens</td>
        <td>$5.00 / 1M tokens</td>
        <td>—</td>
      </tr>
    </tbody>
  </table>
  <p style="font-size:0.9em;text-align:center;margin-top:5px;">
    <em>Pricing as of January 2026. Third-party rates may vary.</em>
  </p>
</div>
<p>
  For cost comparison: Qwen3-Max-Thinking is significantly cheaper than GPT-5 ($15/$60 per 1M input/output) and comparable to Claude Opus ($15/$75). The Novita AI option at $0.50 input makes it one of the most affordable frontier-class models available.
</p>

<h2 id="use-cases">Best Use Cases</h2>
<div class="qwen-grid qwen-grid-cols-2">
  <div class="feature-item"><strong>Math &amp; STEM Research</strong> — Perfect AIME scores and #1 IMO performance make it the top choice for mathematical reasoning and scientific problem-solving.</div>
  <div class="feature-item"><strong>Competitive Coding</strong> — 91.4 on LiveCodeBench v6 with test-time scaling. Ideal for algorithm contests and complex code generation.</div>
  <div class="feature-item"><strong>Deep Research</strong> — The 58.3 HLE-with-search score (far ahead of competitors) shows exceptional ability to combine reasoning with tool use for research tasks.</div>
  <div class="feature-item"><strong>Knowledge-Intensive QA</strong> — 92.8 MMLU-Redux and 93.7 C-Eval demonstrate broad knowledge across domains and languages.</div>
  <div class="feature-item"><strong>Complex Multi-Step Agents</strong> — Native tool calling + test-time scaling enable agents that reason deeply before acting.</div>
  <div class="feature-item"><strong>High-Quality Chat</strong> — 90.2 Arena-Hard v2 and ~1430 LMArena ELO for premium conversational applications.</div>
</div>

<h2 id="limitations">Limitations</h2>
<ul>
  <li><strong>Closed source:</strong> Unlike the rest of the Qwen 3 family, Qwen3-Max is proprietary. You cannot download, inspect, fine-tune, or self-host the weights.</li>
  <li><strong>Speed:</strong> At ~38 tokens/second, it's significantly slower than lighter models. Test-time scaling adds further latency for reasoning-heavy queries.</li>
  <li><strong>Real-world coding:</strong> SWE-Bench Verified at 75.3 means it trails top competitors on multi-file debugging and real software engineering tasks. For dedicated coding, consider <a href="/qwen3-coder/">Qwen3-Coder-Next</a>.</li>
  <li><strong>Agent tool use:</strong> Tau2-Bench at 82.1 is strong but not best-in-class. Claude and Gemini currently handle complex tool chains more reliably.</li>
  <li><strong>Vendor lock-in:</strong> API-only access means you're dependent on Alibaba Cloud's infrastructure, pricing decisions, and uptime.</li>
  <li><strong>Text-only:</strong> Qwen3-Max-Thinking handles text only — no image, audio, or video inputs. For multimodal tasks, look at Qwen3-Omni or Qwen3-VL.</li>
</ul>

<h2 id="timeline">Qwen3-Max Timeline</h2>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <tbody>
      <tr><td><strong>September 5, 2025</strong></td><td>Qwen3-Max launched — 1T+ MoE, API-only, ~1430 LMArena ELO</td></tr>
      <tr><td><strong>November 2025</strong></td><td>Thinking mode added to Qwen3-Max</td></tr>
      <tr><td><strong>January 27, 2026</strong></td><td>Qwen3-Max-Thinking with test-time scaling — perfect AIME 2025, #1 HLE with search</td></tr>
    </tbody>
  </table>
</div>

<h2 id="faq">Frequently Asked Questions</h2>

<h3>Is Qwen3-Max open source?</h3>
<p>
  No. Qwen3-Max is the <strong>only proprietary model</strong> in the Qwen 3 family. It's available exclusively through API. All other Qwen 3 models (0.6B through 235B, Coder, ASR, TTS, etc.) are open-weight under Apache 2.0.
</p>

<h3>Can I run Qwen3-Max locally?</h3>
<p>
  No — the model weights are not publicly available. For the most powerful self-hostable option, use <a href="/qwen-3/">Qwen3-235B-A22B-Thinking-2507</a>, which is open-source and achieves strong benchmark results.
</p>

<h3>What's the difference between Qwen3-Max and Qwen3-Max-Thinking?</h3>
<p>
  They're the same model. <strong>Qwen3-Max</strong> refers to the base model (September 2025). <strong>Qwen3-Max-Thinking</strong> refers to the January 2026 upgrade that added test-time scaling for deeper reasoning. The API model ID <code>qwen3-max-2026-01-23</code> includes both modes — toggle with <code>enable_thinking</code>.
</p>

<h3>How does the pricing compare to GPT-5 and Claude?</h3>
<p>
  Qwen3-Max-Thinking at $1.20/$6.00 per million tokens (input/output) is <strong>significantly cheaper</strong> than GPT-5 and Claude Opus for comparable benchmark performance. Third-party providers like Novita AI offer even lower rates at $0.50/$5.00.
</p>

<h3>Is it good for coding?</h3>
<p>
  Mixed. It leads <strong>LiveCodeBench v6</strong> (91.4) for competitive coding, but trails on <strong>SWE-Bench Verified</strong> (75.3) for real-world software engineering. For dedicated coding tasks, <a href="/qwen3-coder/">Qwen3-Coder-Next</a> is a better specialized choice.
</p>
`;
---
<BaseLayout title="Qwen3-Max-Thinking" seoTitle="Qwen3-Max-Thinking: Alibaba's 1T+ Frontier Model (Benchmarks & API)" seoDescription="Complete guide to Qwen3-Max-Thinking: 1T+ parameter MoE, test-time scaling, perfect AIME score, benchmarks vs GPT-5 & Gemini, API access and pricing.">
  <article class="qwen-container">
    <h1>Qwen3-Max-Thinking</h1>
    <Fragment set:html={rawHtml} />
  </article>
</BaseLayout>
