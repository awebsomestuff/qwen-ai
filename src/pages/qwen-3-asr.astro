---
import BaseLayout from '../layouts/BaseLayout.astro';
const rawHtml = `
<p>
  <strong>Qwen3-ASR</strong> is Alibaba Cloud's open-source automatic speech recognition system, released on <strong>January 30, 2026</strong>. It supports <strong>52 languages and dialects</strong> (including 22 Chinese dialects), handles both streaming and offline transcription from a single unified model, and achieves state-of-the-art accuracy across Chinese, multilingual, and even singing voice benchmarks. The entire family is released under the <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="noopener noreferrer">Apache 2.0 license</a>, making it one of the most capable open-source ASR systems available today.
</p>
<div class="ad-afterintro-container"><div class="ad-afterintro-inner"><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-9609544329602409" data-ad-slot="8310388095" data-ad-format="auto" data-full-width-responsive="true"></ins></div></div>

<div class="qwen-container" style="text-align:center;margin-bottom:30px;">
  <div class="qwen-row" style="justify-content:center;gap:12px;flex-wrap:wrap;">
    <div class="qwen-col">
      <a class="qwen-button pro" href="https://huggingface.co/collections/Qwen/qwen3-asr" target="_blank" rel="nofollow noopener noreferrer">
        <span class="button-content"><span class="button-text">Qwen3-ASR on Hugging Face</span></span>
      </a>
    </div>
    <div class="qwen-col">
      <a class="qwen-button primary" href="https://github.com/QwenLM/Qwen3-ASR" target="_blank" rel="nofollow noopener noreferrer">
        <span class="button-content"><span class="button-text">GitHub Repository</span></span>
      </a>
    </div>
  </div>
</div>

<p>
  What sets Qwen3-ASR apart from alternatives like Whisper or GPT-4o Transcribe isn't just raw accuracy &mdash; it's the combination of capabilities packed into a single lightweight model. It can transcribe noisy environments, recognize singing voices over background music, provide word-level timestamps via its companion ForcedAligner model, and switch seamlessly between streaming and offline modes. For context on the broader model family, see the <a href="/qwen-3/">Qwen 3 overview</a>.
</p>

<figure style="margin:2em 0;text-align:center;">
  <img src="/wp-content/uploads/2026/02/qwen3-asr-architecture.webp" alt="Qwen3-ASR capabilities overview showing robustness in noisy environments, timestamp prediction, singing voice recognition, multilingual support, and fast streaming inference" width="900" height="502" loading="lazy" style="max-width:100%;height:auto;border-radius:8px;" />
  <figcaption style="margin-top:0.5em;font-size:0.9em;color:#6b7280;">Qwen3-ASR capabilities: noise robustness, forced alignment timestamps, singing voice transcription, multilingual code-switching, and real-time streaming.</figcaption>
</figure>

<div class="qwen-toc">
  <p class="qwen-toc-label">In This Guide</p>
  <div class="qwen-toc-grid">
    <a href="#model-variants" class="qwen-toc-chip">Model Variants</a>
    <a href="#architecture" class="qwen-toc-chip">Architecture</a>
    <a href="#supported-languages" class="qwen-toc-chip">52 Languages</a>
    <a href="#benchmarks" class="qwen-toc-chip">Benchmarks</a>
    <a href="#unique-features" class="qwen-toc-chip">Key Features</a>
    <a href="#run-locally" class="qwen-toc-chip qwen-toc-highlight">Run Locally</a>
    <a href="#cloud-api" class="qwen-toc-chip">Cloud API</a>
    <a href="#forced-aligner" class="qwen-toc-chip">ForcedAligner</a>
    <a href="#vs-whisper" class="qwen-toc-chip">vs. Whisper</a>
    <a href="#conclusion" class="qwen-toc-chip">Verdict</a>
  </div>
</div>

<h2 id="model-variants">Model Variants</h2>
<p>
  The Qwen3-ASR family includes three open-source models and one cloud-only API variant:
</p>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Model</th>
        <th>Parameters</th>
        <th>Type</th>
        <th>License</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="https://huggingface.co/Qwen/Qwen3-ASR-1.7B" target="_blank" rel="nofollow noopener noreferrer"><strong>Qwen3-ASR-1.7B</strong></a></td>
        <td>~2B (1.7B LLM + encoder + projector)</td>
        <td>Best quality, offline & streaming</td>
        <td>Apache 2.0</td>
      </tr>
      <tr>
        <td><a href="https://huggingface.co/Qwen/Qwen3-ASR-0.6B" target="_blank" rel="nofollow noopener noreferrer"><strong>Qwen3-ASR-0.6B</strong></a></td>
        <td>~0.9B (0.6B LLM + encoder + projector)</td>
        <td>Faster, lower VRAM</td>
        <td>Apache 2.0</td>
      </tr>
      <tr>
        <td><a href="https://huggingface.co/Qwen/Qwen3-ForcedAligner-0.6B" target="_blank" rel="nofollow noopener noreferrer"><strong>Qwen3-ForcedAligner-0.6B</strong></a></td>
        <td>~0.9B</td>
        <td>Word-level timestamp alignment</td>
        <td>Apache 2.0</td>
      </tr>
      <tr>
        <td><strong>Qwen3-ASR-Flash</strong> (API only)</td>
        <td>Not disclosed</td>
        <td>Cloud real-time API via DashScope</td>
        <td>Proprietary</td>
      </tr>
    </tbody>
  </table>
</div>
<p>
  The 1.7B variant is the flagship for accuracy, while the 0.6B model offers a compelling trade-off &mdash; achieving a <strong>real-time factor (RTF) of 0.064</strong>, meaning it can transcribe roughly 2,000 seconds of speech per second at high concurrency. Both models share the same architecture and support the same 52 languages.
</p>

<h2 id="architecture">Architecture: Audio Transformer + Qwen3 LLM</h2>
<p>
  Qwen3-ASR uses a three-component architecture built on top of the Qwen3-Omni foundation:
</p>
<ul>
  <li><strong>AuT Encoder</strong> &mdash; A pretrained Audio Transformer that performs 8x downsampling on Fbank features (128 dimensions), producing a 12.5Hz token rate. The 1.7B variant uses a 300M-parameter encoder with 1024 hidden size; the 0.6B variant uses 180M parameters with 896 hidden size.</li>
  <li><strong>Projector</strong> &mdash; Bridges the audio encoder's representations to the LLM decoder's embedding space.</li>
  <li><strong>LLM Decoder</strong> &mdash; A Qwen3-1.7B or Qwen3-0.6B language model that generates the text output autoregressively.</li>
</ul>
<figure style="margin:2em 0;text-align:center;">
  <img src="/wp-content/uploads/2026/02/qwen3-asr-streaming.webp" alt="Qwen3-ASR architecture diagram showing AuT Encoder with downsampling Conv2d and self-attention layers, projector, and Qwen3 LM decoder" width="900" height="329" loading="lazy" style="max-width:100%;height:auto;border-radius:8px;" />
  <figcaption style="margin-top:0.5em;font-size:0.9em;color:#6b7280;">Qwen3-ASR architecture: AuT Encoder (left) processes audio into tokens, which the Qwen3 LM decoder (right) converts to text.</figcaption>
</figure>
<p>
  A key innovation is the <strong>dynamic flash attention window</strong> that ranges from 1 to 8 seconds. This allows a single model to handle both streaming (low-latency, real-time) and offline (full-context, higher accuracy) inference without needing separate model weights.
</p>

<h3>Training Pipeline</h3>
<p>
  The model was trained through a rigorous four-stage pipeline:
</p>
<ol>
  <li><strong>AuT Pretraining</strong> &mdash; Approximately <strong>40 million hours</strong> of pseudo-labeled ASR data, primarily Chinese and English.</li>
  <li><strong>Omni Pretraining</strong> &mdash; Multi-task training with <strong>3 trillion tokens</strong> covering audio, vision, and text (as part of the Qwen3-Omni foundation).</li>
  <li><strong>ASR Supervised Fine-Tuning</strong> &mdash; Targeted training on the ASR format with multilingual data, streaming enhancements, and context-biasing data.</li>
  <li><strong>Reinforcement Learning (GSPO)</strong> &mdash; Group Sequence Policy Optimization with ~50,000 utterances to refine output quality.</li>
</ol>

<h2 id="supported-languages">Supported Languages: 52 Total</h2>
<p>
  Qwen3-ASR supports 30 languages and 22 Chinese dialects &mdash; by far the most comprehensive coverage among open-source ASR models:
</p>
<div class="qwen-grid qwen-grid-cols-2">
  <div class="feature-item">
    <strong>30 Languages</strong>: Chinese, English, Cantonese, Arabic, German, French, Spanish, Portuguese, Indonesian, Italian, Korean, Russian, Thai, Vietnamese, Japanese, Turkish, Hindi, Malay, Dutch, Swedish, Danish, Finnish, Polish, Czech, Filipino, Persian, Greek, Hungarian, Macedonian, Romanian
  </div>
  <div class="feature-item">
    <strong>22 Chinese Dialects</strong>: Anhui, Dongbei, Fujian, Gansu, Guizhou, Hebei, Henan, Hubei, Hunan, Jiangxi, Ningxia, Shandong, Shaanxi, Shanxi, Sichuan, Tianjin, Yunnan, Zhejiang, Cantonese (HK/Guangdong), Wu, Minnan, and more
  </div>
</div>
<p>
  The model also handles <strong>multilingual code-switching</strong> within a single utterance &mdash; for example, a speaker mixing English and Chinese in the same sentence without any manual language selection.
</p>

<h2 id="benchmarks">Benchmarks & Performance</h2>
<p>
  Qwen3-ASR-1.7B achieves state-of-the-art results across multiple categories, significantly outperforming Whisper-large-v3 and GPT-4o-Transcribe in Chinese and multilingual benchmarks:
</p>

<h3>English ASR (Word Error Rate &mdash; lower is better)</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Dataset</th>
        <th>Qwen3-ASR-1.7B</th>
        <th>Whisper-large-v3</th>
        <th>GPT-4o-Transcribe</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>LibriSpeech (clean/other)</td>
        <td>1.63 / 3.38</td>
        <td>1.51 / 3.97</td>
        <td>1.39 / 3.75</td>
      </tr>
      <tr>
        <td>GigaSpeech</td>
        <td><strong>8.45</strong></td>
        <td>9.76</td>
        <td>&mdash;</td>
      </tr>
      <tr>
        <td>CommonVoice-en</td>
        <td><strong>7.39</strong></td>
        <td>9.90</td>
        <td>&mdash;</td>
      </tr>
    </tbody>
  </table>
</div>

<h3>Chinese ASR (Character Error Rate &mdash; lower is better)</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Dataset</th>
        <th>Qwen3-ASR-1.7B</th>
        <th>Whisper-large-v3</th>
        <th>GPT-4o-Transcribe</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>WenetSpeech (net/meeting)</td>
        <td><strong>4.97 / 5.88</strong></td>
        <td>9.86 / 19.11</td>
        <td>15.30 / 32.27</td>
      </tr>
      <tr>
        <td>AISHELL-2 test</td>
        <td><strong>2.71</strong></td>
        <td>&mdash;</td>
        <td>4.24</td>
      </tr>
      <tr>
        <td>SpeechIO</td>
        <td><strong>2.88</strong></td>
        <td>&mdash;</td>
        <td>12.86</td>
      </tr>
    </tbody>
  </table>
</div>

<h3>Singing Voice & Background Music (WER)</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Dataset</th>
        <th>Qwen3-ASR-1.7B</th>
        <th>GPT-4o</th>
        <th>Doubao-ASR</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>M4Singer</td>
        <td><strong>5.98</strong></td>
        <td>16.77</td>
        <td>7.88</td>
      </tr>
      <tr>
        <td>EntireSongs-en</td>
        <td><strong>14.60</strong></td>
        <td>30.71</td>
        <td>33.51</td>
      </tr>
      <tr>
        <td>EntireSongs-zh</td>
        <td><strong>13.91</strong></td>
        <td>34.86</td>
        <td>23.99</td>
      </tr>
    </tbody>
  </table>
</div>
<p>
  The singing voice results are particularly impressive &mdash; Qwen3-ASR achieves sub-6% WER on solo singing and sub-15% even on full songs with strong background music, dramatically outperforming all competitors.
</p>

<figure style="margin:2em 0;text-align:center;">
  <img src="/wp-content/uploads/2026/02/qwen3-asr-benchmarks.webp" alt="Qwen3-ASR-Flash error rates bar chart comparing performance against Gemini-2.5-Pro, GPT-4o-Transcribe, Paraformer-v2, and Doubao-ASR across Chinese, English, Multilingual, Entities, and Lyrics benchmarks" width="900" height="528" loading="lazy" style="max-width:100%;height:auto;border-radius:8px;" />
  <figcaption style="margin-top:0.5em;font-size:0.9em;color:#6b7280;">Qwen3-ASR-Flash error rates vs. Gemini-2.5-Pro, GPT-4o-Transcribe, and others across multiple benchmark categories.</figcaption>
</figure>

<figure style="margin:2em 0;text-align:center;">
  <img src="/wp-content/uploads/2026/02/qwen3-asr-languages.webp" alt="Comprehensive benchmark table showing Qwen3-ASR-1.7B word error rates and character error rates across multilingual, English, Chinese, Chinese dialect, singing, and songs with background music datasets compared to GPT-4o-Transcribe, Gemini-2.5-Pro, Doubao-ASR, and Whisper-large-V3" width="900" height="1100" loading="lazy" style="max-width:100%;height:auto;border-radius:8px;" />
  <figcaption style="margin-top:0.5em;font-size:0.9em;color:#6b7280;">Full benchmark results: Qwen3-ASR-1.7B across all test categories vs. GPT-4o-Transcribe, Gemini-2.5-Pro, Doubao-ASR, and Whisper-large-V3.</figcaption>
</figure>

<h2 id="unique-features">Key Differentiating Features</h2>

<figure style="margin:2em 0;text-align:center;">
  <img src="/wp-content/uploads/2026/02/qwen3-asr-features.webp" alt="Qwen3-ASR-Flash illustrated features: robust speech recognition in complex acoustic environments, accurate singing voice transcription, multilingual code-switching support, and customizable text prompting" width="900" height="545" loading="lazy" style="max-width:100%;height:auto;border-radius:8px;" />
  <figcaption style="margin-top:0.5em;font-size:0.9em;color:#6b7280;">Qwen3-ASR-Flash key capabilities: noise robustness, singing transcription, 11+ languages with code-switching, and prompt-based customization.</figcaption>
</figure>

<div class="skills-section">
<div class="skills-grid">
<div class="skill-steps">
<h3>1. Unified Streaming & Offline Model</h3>
<p>
  Most ASR models force you to choose between a streaming model (low latency, lower accuracy) and an offline model (higher accuracy, higher latency). Qwen3-ASR uses a <strong>dynamic flash attention window</strong> (1&ndash;8 seconds) that lets a single set of weights handle both modes. In streaming mode, the 1.7B model maintains a WER of 4.51 on LibriSpeech-other vs. 3.38 in offline &mdash; a very small accuracy trade-off for real-time capability.
</p>
</div>
</div>
</div>

<div class="skills-section">
<div class="skills-grid">
<div class="skill-steps">
<h3>2. Context Biasing</h3>
<p>
  This is a feature that most open-source ASR models lack entirely. You can provide arbitrary text (up to <strong>10,000 tokens</strong> in the API) to bias the transcription toward specific terminology &mdash; company names, medical terms, legal jargon, product codes, or any domain-specific vocabulary. The model will prefer these terms when the audio is ambiguous, dramatically improving accuracy in specialized domains.
</p>
</div>
</div>
</div>

<div class="skills-section">
<div class="skills-grid">
<div class="skill-steps">
<h3>3. Singing Voice & Music Recognition</h3>
<p>
  Qwen3-ASR can accurately transcribe singing voices even with strong background music &mdash; something that destroys the accuracy of virtually every other ASR model. It achieves under 6% WER on solo singing benchmarks and under 15% on full songs with instrumental accompaniment, outperforming GPT-4o by more than 2x.
</p>
</div>
</div>
</div>

<div class="skills-section">
<div class="skills-grid">
<div class="skill-steps">
<h3>4. Inverse Text Normalization</h3>
<p>
  The model automatically converts spoken forms to clean written text: numbers, dates, currencies, email addresses, and URLs are formatted properly without any post-processing needed.
</p>
</div>
</div>
</div>

<h2 id="run-locally">How to Run Qwen3-ASR Locally</h2>
<p>
  The Qwen team provides an official Python package that makes deployment straightforward. There are two backends: a basic Transformers backend and a high-performance vLLM backend.
</p>

<div class="skills-section">
<div class="skills-grid">
<div class="skill-steps">
<h3>Option 1: Quick Install (Transformers Backend)</h3>
<p>The simplest way to get started for testing and light usage.</p>
<ol>
  <li><strong>Install the package:</strong>
    <div class="command-box" style="padding: 1rem; background: #f5f5f5; border-radius: 4px; margin: 0.5em 0;"><code>pip install -U qwen-asr</code></div>
  </li>
  <li><strong>Launch the web demo:</strong>
    <div class="command-box" style="padding: 1rem; background: #f5f5f5; border-radius: 4px; margin: 0.5em 0;"><code>qwen-asr-demo Qwen/Qwen3-ASR-1.7B</code></div>
  </li>
  <li><strong>Or use the streaming demo:</strong>
    <div class="command-box" style="padding: 1rem; background: #f5f5f5; border-radius: 4px; margin: 0.5em 0;"><code>qwen-asr-demo-streaming Qwen/Qwen3-ASR-1.7B</code></div>
  </li>
</ol>
</div>
</div>
</div>

<div class="skills-section">
<div class="skills-grid">
<div class="skill-steps">
<h3>Option 2: vLLM Backend (Recommended for Speed)</h3>
<p>For production use or when you need high throughput.</p>
<ol>
  <li><strong>Install with vLLM support:</strong>
    <div class="command-box" style="padding: 1rem; background: #f5f5f5; border-radius: 4px; margin: 0.5em 0;"><code>pip install -U qwen-asr[vllm]</code></div>
  </li>
  <li><strong>Launch the OpenAI-compatible server:</strong>
    <div class="command-box" style="padding: 1rem; background: #f5f5f5; border-radius: 4px; margin: 0.5em 0;"><code>qwen-asr-serve Qwen/Qwen3-ASR-1.7B --port 8000</code></div>
  </li>
</ol>
<p>This exposes an OpenAI-compatible API endpoint, enabling drop-in replacement in existing workflows.</p>
</div>
</div>
</div>

<div class="skills-section">
<div class="skills-grid">
<div class="skill-steps">
<h3>Option 3: Docker</h3>
<p>For containerized deployments with all dependencies pre-installed.</p>
<div class="command-box" style="padding: 1rem; background: #f5f5f5; border-radius: 4px; margin: 0.5em 0;"><code>docker run --gpus all --shm-size=4gb -p 8000:8000 qwenllm/qwen3-asr:latest</code></div>
</div>
</div>
</div>

<h3>Supported Audio Inputs</h3>
<p>
  Qwen3-ASR accepts a wide variety of input formats:
</p>
<ul>
  <li>Local files &mdash; MP3, WAV, M4A, MP4, MOV, MKV, and more (via FFmpeg)</li>
  <li>HTTP/HTTPS URLs (direct audio links)</li>
  <li>Base64-encoded audio data</li>
  <li>NumPy arrays with sample rate tuples</li>
</ul>
<p>All inputs are automatically resampled to 16kHz mono.</p>

<h3>Hardware Requirements</h3>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Model</th>
        <th>Min VRAM</th>
        <th>Recommended</th>
        <th>RTF (Concurrency 128)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Qwen3-ASR-1.7B</td>
        <td>~4 GB (weights only)</td>
        <td>16 GB+ for batch inference</td>
        <td>&mdash;</td>
      </tr>
      <tr>
        <td>Qwen3-ASR-0.6B</td>
        <td>~2 GB (weights only)</td>
        <td>8 GB+ for batch inference</td>
        <td>0.064</td>
      </tr>
    </tbody>
  </table>
</div>
<p>
  For more details on running Qwen models locally, check our <a href="/download-models-locally/">guide to running Qwen locally</a> and our <a href="/requirements/">hardware requirements page</a>.
</p>

<h2 id="cloud-api">Cloud API: Qwen3-ASR-Flash</h2>
<p>
  For users who don't want to self-host, Alibaba offers Qwen3-ASR-Flash as a real-time WebSocket API through the DashScope platform. Key details:
</p>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <tbody>
      <tr>
        <td><strong>Protocol</strong></td>
        <td>WebSocket (WSS)</td>
      </tr>
      <tr>
        <td><strong>Endpoint (International)</strong></td>
        <td><code>wss://dashscope-intl.aliyuncs.com/api-ws/v1/realtime</code></td>
      </tr>
      <tr>
        <td><strong>Pricing (International)</strong></td>
        <td>$0.00009/second (~$0.32/hour)</td>
      </tr>
      <tr>
        <td><strong>Rate Limit</strong></td>
        <td>20 requests per second</td>
      </tr>
      <tr>
        <td><strong>Audio Formats</strong></td>
        <td>PCM, Opus (8 kHz or 16 kHz, mono)</td>
      </tr>
      <tr>
        <td><strong>Features</strong></td>
        <td>Real-time streaming, emotion recognition, context biasing</td>
      </tr>
    </tbody>
  </table>
</div>
<p>
  The API-only variant also supports features not yet available in the open-source models, including <strong>emotion recognition</strong> in real-time.
</p>

<h3>Qwen3-ASR-Toolkit (CLI for Long Audio)</h3>
<p>
  The official <a href="https://github.com/QwenLM/Qwen3-ASR-Toolkit" target="_blank" rel="nofollow noopener noreferrer">Qwen3-ASR-Toolkit</a> provides a command-line tool that overcomes the API's 3-minute audio limit through intelligent chunking:
</p>
<div class="command-box" style="padding: 1rem; background: #f5f5f5; border-radius: 4px; margin: 0.5em 0;"><code>pip install qwen3-asr-toolkit<br/>qwen3-asr -i audio_file.mp3 -j 4 --save-srt</code></div>
<p>
  It handles automatic resampling, Voice Activity Detection (VAD) for smart splitting, parallel processing across multiple threads, SRT subtitle generation, and hallucination removal post-processing.
</p>

<h2 id="forced-aligner">Qwen3-ForcedAligner: Millisecond-Precision Timestamps</h2>
<p>
  The companion <strong>Qwen3-ForcedAligner-0.6B</strong> model provides word and character-level timestamps with exceptional precision. It uses a non-autoregressive (NAR) architecture with a "slot-filling" approach to timestamp prediction.
</p>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Language</th>
        <th>Qwen3-ForcedAligner</th>
        <th>WhisperX</th>
        <th>NFA</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Chinese</td>
        <td><strong>33.1 ms</strong></td>
        <td>&mdash;</td>
        <td>109.8 ms</td>
      </tr>
      <tr>
        <td>English</td>
        <td><strong>37.5 ms</strong></td>
        <td>92.1 ms</td>
        <td>107.5 ms</td>
      </tr>
      <tr>
        <td>French</td>
        <td><strong>41.7 ms</strong></td>
        <td>145.3 ms</td>
        <td>100.7 ms</td>
      </tr>
      <tr>
        <td>German</td>
        <td><strong>46.5 ms</strong></td>
        <td>165.1 ms</td>
        <td>122.7 ms</td>
      </tr>
      <tr>
        <td>Japanese</td>
        <td><strong>42.2 ms</strong></td>
        <td>&mdash;</td>
        <td>&mdash;</td>
      </tr>
      <tr>
        <td><strong>Average</strong></td>
        <td><strong>42.9 ms</strong></td>
        <td>133.2 ms</td>
        <td>129.8 ms</td>
      </tr>
    </tbody>
  </table>
</div>
<p>
  The ForcedAligner achieves a <strong>67&ndash;77% reduction in alignment error</strong> compared to WhisperX and NFA, averaging just 42.9ms absolute shift. It supports 11 languages: Chinese, English, Cantonese, French, German, Italian, Japanese, Korean, Portuguese, Russian, and Spanish. This makes it ideal for subtitle generation, karaoke timing, audio editing, and any application requiring precise word-level synchronization.
</p>

<h2 id="vs-whisper">Qwen3-ASR vs. Whisper vs. GPT-4o Transcribe</h2>
<p>
  How does Qwen3-ASR stack up against the most popular alternatives?
</p>
<div class="qwen-table-wrapper">
  <table class="qwen-table">
    <thead>
      <tr>
        <th>Feature</th>
        <th>Qwen3-ASR-1.7B</th>
        <th>Whisper-large-v3</th>
        <th>GPT-4o-Transcribe</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Open-source</strong></td>
        <td>Yes (Apache 2.0)</td>
        <td>Yes</td>
        <td>No (API only)</td>
      </tr>
      <tr>
        <td><strong>Languages</strong></td>
        <td>52 (30 + 22 dialects)</td>
        <td>99</td>
        <td>~50+</td>
      </tr>
      <tr>
        <td><strong>Chinese performance</strong></td>
        <td>SOTA (2&ndash;3x better)</td>
        <td>Moderate</td>
        <td>Weak</td>
      </tr>
      <tr>
        <td><strong>English performance</strong></td>
        <td>Competitive</td>
        <td>Strong</td>
        <td>Strong</td>
      </tr>
      <tr>
        <td><strong>Singing/BGM</strong></td>
        <td>SOTA (&lt;6% WER solo)</td>
        <td>Poor</td>
        <td>Poor</td>
      </tr>
      <tr>
        <td><strong>Streaming</strong></td>
        <td>Yes (unified model)</td>
        <td>No</td>
        <td>Yes</td>
      </tr>
      <tr>
        <td><strong>Forced alignment</strong></td>
        <td>Yes (42.9ms avg)</td>
        <td>Via WhisperX (133ms)</td>
        <td>No</td>
      </tr>
      <tr>
        <td><strong>Context biasing</strong></td>
        <td>Yes (10K tokens)</td>
        <td>No</td>
        <td>No</td>
      </tr>
      <tr>
        <td><strong>Self-hostable</strong></td>
        <td>Yes</td>
        <td>Yes</td>
        <td>No</td>
      </tr>
    </tbody>
  </table>
</div>
<p>
  Whisper still leads in raw language count (99 vs. 52), and GPT-4o edges ahead slightly on clean English benchmarks like LibriSpeech. But Qwen3-ASR dominates in Chinese, multilingual robustness, singing voice recognition, streaming capability, and the forced alignment use case. For most real-world applications &mdash; especially those involving noisy audio, multiple languages, or specialized terminology &mdash; Qwen3-ASR is the stronger choice. If you want to compare Qwen models against other LLMs more broadly, check our <a href="/vs-chatgpt/">Qwen vs. ChatGPT</a> comparison.
</p>

<h2 id="conclusion">Final Verdict</h2>
<p>
  Qwen3-ASR is a major step forward for open-source speech recognition. It doesn't just match commercial alternatives &mdash; it surpasses them in key areas like Chinese transcription, singing voice recognition, and word-level alignment precision. The unified streaming/offline design, context biasing capability, and the lightweight model sizes make it practical for everything from personal projects to production deployments.
</p>
<p>
  The Apache 2.0 license removes any commercial restrictions, and the official Python package makes getting started as simple as <code>pip install qwen-asr</code>. Whether you're building a transcription service, adding speech input to an application, or creating subtitles, Qwen3-ASR deserves serious consideration.
</p>
<p>
  For the companion text-to-speech models, see our <a href="/qwen-3-tts/">Qwen3-TTS guide</a>. Explore the full <a href="/qwen-3/">Qwen 3 family</a>, try <a href="/chat/">Qwen AI Chat</a>, or check our <a href="/download-models-locally/">guide to running Qwen models locally</a>.
</p>
`;
---
<BaseLayout title="Qwen3-ASR" seoTitle="Qwen3-ASR â€” Open-Source Speech Recognition (52 Languages)" seoDescription="Complete guide to Qwen3-ASR: 52-language ASR with streaming, singing voice recognition, forced alignment, and context biasing. Apache 2.0, run locally.">
  <article class="qwen-container">
    <h1>Qwen3-ASR: Alibaba's Open-Source Speech Recognition System</h1>
    <Fragment set:html={rawHtml} />
  </article>
</BaseLayout>
